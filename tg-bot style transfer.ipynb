{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ljfS1woARWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "4ec00fcf-84f3-4c13-c377-c6999c420a62"
      },
      "source": [
        "!pip install aiogram==1.2.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiogram==1.2.3 in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: aiohttp>=2.3.5 in /usr/local/lib/python3.6/dist-packages (from aiogram==1.2.3) (3.6.2)\n",
            "Requirement already satisfied: certifi>=2018.01.18 in /usr/local/lib/python3.6/dist-packages (from aiogram==1.2.3) (2020.6.20)\n",
            "Requirement already satisfied: Babel>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from aiogram==1.2.3) (2.8.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (19.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (1.4.2)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (4.7.6)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (3.0.4)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (1.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=2.3.5->aiogram==1.2.3) (3.6.6)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from Babel>=2.5.1->aiogram==1.2.3) (2018.9)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp>=2.3.5->aiogram==1.2.3) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYvu-z7iDKd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bdf2c7e2-e0fa-4e5c-e3c5-4157dc5998db"
      },
      "source": [
        "!pip install vedis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vedis in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from vedis) (0.29.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdy3Bxdq_2aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "        def __init__(self, target,):\n",
        "            super(ContentLoss, self).__init__()\n",
        "            self.target = target.detach()#это константа. Убираем ее из дерева вычеслений\n",
        "            self.loss = F.mse_loss(self.target, self.target )#to initialize with something\n",
        "\n",
        "        def forward(self, input):\n",
        "            self.loss = F.mse_loss(input, self.target)\n",
        "            return input\n",
        "        \n",
        "class StyleLoss(nn.Module):\n",
        "        def __init__(self, target_feature):\n",
        "            super(StyleLoss, self).__init__()\n",
        "            self.target = gram_matrix(target_feature).detach()\n",
        "            self.loss = F.mse_loss(self.target, self.target)# to initialize with something\n",
        "            \n",
        "        def gram_matrix(input):\n",
        "            batch_size , h, w, f_map_num = input.size()  # batch size(=1)\n",
        "            # b=number of feature maps\n",
        "            # (h,w)=dimensions of a feature map (N=h*w)\n",
        "    \n",
        "            features = input.view(batch_size * h, w * f_map_num)  # resise F_XL into \\hat F_XL\n",
        "    \n",
        "            G = torch.mm(features, features.t())  # compute the gram product\n",
        "    \n",
        "            # we 'normalize' the values of the gram matrix\n",
        "            # by dividing by the number of element in each feature maps.\n",
        "            return G.div(batch_size * h * w * f_map_num)\n",
        "\n",
        "        def forward(self, input):\n",
        "            G = gram_matrix(input)\n",
        "            self.loss = F.mse_loss(G, self.target)\n",
        "            return input\n",
        "        \n",
        "        \n",
        "class Normalization(nn.Module):\n",
        "        def __init__(self, mean, std):\n",
        "            super(Normalization, self).__init__()\n",
        "            # .view the mean and std to make them [C x 1 x 1] so that they can\n",
        "            # directly work with image Tensor of shape [B x C x H x W].\n",
        "            # B is batch size. C is number of channels. H is height and W is width.\n",
        "            self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "            self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "        def forward(self, img):\n",
        "            # normalize img\n",
        "            return (img - self.mean) / self.std\n",
        "\n",
        "def gram_matrix(input):\n",
        "        batch_size , h, w, f_map_num = input.size()  # batch size(=1)\n",
        "        # b=number of feature maps\n",
        "        # (h,w)=dimensions of a feature map (N=h*w)\n",
        "\n",
        "        features = input.view(batch_size * h, w * f_map_num)  # resise F_XL into \\hat F_XL\n",
        "\n",
        "        G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "        # we 'normalize' the values of the gram matrix\n",
        "        # by dividing by the number of element in each feature maps.\n",
        "        return G.div(batch_size * h * w * f_map_num)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5jFk5-AHII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "        \n",
        "class StyleTransferModel:\n",
        "    def __init__(self,device, cnn,normalization_mean, normalization_std,\n",
        "                 style_img, content_img,\n",
        "                 content_layers=['conv_4'],\n",
        "                 style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
        "        \n",
        "        cnn = copy.deepcopy(cnn)\n",
        "        self.device = device\n",
        "        style_img = self._preprocesses(style_img).to(self.device, torch.float)\n",
        "        content_img = self._preprocesses(content_img).to(self.device, torch.float)\n",
        "\n",
        "        # normalization module\n",
        "        normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "        # just in order to have an iterable access to or list of content/syle\n",
        "        # losses\n",
        "        content_losses = []\n",
        "        style_losses = []\n",
        "\n",
        "        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
        "        # to put in modules that are supposed to be activated sequentially\n",
        "        model = nn.Sequential(normalization)\n",
        "\n",
        "        i = 0  # increment every time we see a conv\n",
        "        for layer in cnn.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                i += 1\n",
        "                name = 'conv_{}'.format(i)\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = 'relu_{}'.format(i)\n",
        "                layer = nn.ReLU(inplace=False)\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = 'pool_{}'.format(i)\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = 'bn_{}'.format(i)\n",
        "            else:\n",
        "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            if name in content_layers:\n",
        "                # add content loss:\n",
        "                target = model(content_img).detach()\n",
        "                content_loss = ContentLoss(target)\n",
        "                model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "                content_losses.append(content_loss)\n",
        "\n",
        "            if name in style_layers:\n",
        "                # add style loss:\n",
        "                target_feature = model(style_img).detach()\n",
        "                style_loss = StyleLoss(target_feature)\n",
        "                model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "                style_losses.append(style_loss)\n",
        "\n",
        "        # now we trim off the layers after the last content and style losses\n",
        "        for i in range(len(model) - 1, -1, -1):\n",
        "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "                break\n",
        "\n",
        "        self.model = model[:(i + 1)]\n",
        "        self.style_losses = style_losses\n",
        "        self.content_losses = content_losses\n",
        "    \n",
        "        \n",
        "    def __call__(self, input_img, num_steps=500, style_weight=100000, content_weight = 1):\n",
        "        \n",
        "        input_img = self._preprocesses(input_img).to(self.device,torch.float)\n",
        "        optimizer = optim.LBFGS([input_img.requires_grad_()]) \n",
        "\n",
        "        print('Optimizing..')\n",
        "        run = [0]\n",
        "        while run[0] <= num_steps:\n",
        "\n",
        "            def closure():\n",
        "                # correct the values \n",
        "                # это для того, чтобы значения тензора картинки не выходили за пределы [0;1]\n",
        "                input_img.data.clamp_(0, 1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                self.model(input_img)\n",
        "\n",
        "                style_score = 0\n",
        "                content_score = 0\n",
        "\n",
        "                for sl in self.style_losses:\n",
        "                    style_score += sl.loss\n",
        "                for cl in self.content_losses:\n",
        "                    content_score += cl.loss\n",
        "                \n",
        "                #взвешивание ощибки\n",
        "                style_score *= style_weight\n",
        "                content_score *= content_weight\n",
        "\n",
        "                loss = style_score + content_score\n",
        "                loss.backward()\n",
        "\n",
        "                run[0] += 1\n",
        "                if run[0] % 50 == 0:\n",
        "                    print(\"run {}:\".format(run))\n",
        "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                        style_score.item(), content_score.item()))\n",
        "                    print()\n",
        "\n",
        "                return style_score + content_score\n",
        "\n",
        "            optimizer.step(closure)\n",
        "\n",
        "        # a last correction...\n",
        "        input_img.data.clamp_(0, 1)\n",
        "\n",
        "        return input_img\n",
        "    \n",
        "    def _preprocesses(self, img):\n",
        "        imsize = 128\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize(imsize),  # нормируем размер изображения\n",
        "            transforms.CenterCrop(imsize),\n",
        "            transforms.ToTensor()])\n",
        "        img = loader(img).unsqueeze(0)\n",
        "        return img"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1H7zOFLDax8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "db_file = \"database.vdb\"\n",
        "class States(Enum):\n",
        "    S_START = \"0\"  # Начало нового диалога\n",
        "    S_STYLE = \"1\"\n",
        "    S_CONTENT = \"2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jp0Ae5JDG0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vedis import Vedis\n",
        "\n",
        "# Пытаемся узнать из базы «состояние» пользователя\n",
        "def get_current_state(user_id):\n",
        "    with Vedis(db_file) as db:\n",
        "        try:\n",
        "            return db[user_id].decode() # Если используете Vedis версии ниже, чем 0.7.1, то .decode() НЕ НУЖЕН\n",
        "        except KeyError:  # Если такого ключа почему-то не оказалось\n",
        "            return States.S_START.value  # значение по умолчанию - начало диалога\n",
        "\n",
        "def set_state(user_id, value):\n",
        "    with Vedis(db_file) as db:\n",
        "        try:\n",
        "            db[user_id] = value\n",
        "            return True\n",
        "        except:\n",
        "            return False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0b-Fk18oxmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6e0e38e-9fa6-476f-9d3c-74d26f1cf959"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-7FoqL-2b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from aiogram import Bot, types\n",
        "from aiogram.dispatcher import Dispatcher\n",
        "\n",
        "from aiogram.utils import executor\n",
        "from aiogram.utils.helper import Helper, HelperMode, ListItem\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from aiogram.types import ReplyKeyboardRemove, ReplyKeyboardMarkup, KeyboardButton\n",
        "\n",
        "TOKEN = '1239115079:AAHB2lI3iB2K23fYVC9iZTtRp1XFCp0IWUE'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZsqMcsW_Fhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "button_hi = KeyboardButton('Давай!')\n",
        "greet_kb = ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True).add(button_hi)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbeTd2TzAHm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63b62b50-8ee3-47b1-fc70-300b1f724d90"
      },
      "source": [
        "bot = Bot(token=TOKEN)\n",
        "dp = Dispatcher(bot)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "image1 = False\n",
        "image2 = False\n",
        "\n",
        "@dp.message_handler(commands=['start'])\n",
        "async def process_start_command(message: types.Message):\n",
        "  set_state(message.from_user.id, str(States.S_START.value))\n",
        "  await message.reply(\"Привет!\\nЯ умею переносить стиль на фотографии, давай поиграем!\", \n",
        "                      reply_markup=greet_kb)\n",
        "  \n",
        "@dp.message_handler(func=lambda message: get_current_state(message.chat.id) == States.S_START.value, text = \"Давай!\")\n",
        "async def process_start_command(message: types.Message):\n",
        "  set_state(message.from_user.id, str(States.S_STYLE.value))\n",
        "  await message.reply(\"Отправь мне картинку стиля\")\n",
        "    \n",
        "@dp.message_handler(commands=['help'])\n",
        "async def process_help_command(message: types.Message):\n",
        "    await message.reply(\"Напиши мне что-нибудь, и я отправлю этот текст тебе в ответ!\")\n",
        "\n",
        "@dp.message_handler(func=lambda message: get_current_state(message.chat.id) == States.S_START.value, commands=['start_style_transfer'])    \n",
        "async def process_start_style_command(message: types.Message):\n",
        "    set_state(message.from_user.id, States.S_STYLE.value)\n",
        "    await message.reply(\"Отправь мне картинку стиля\")\n",
        "\n",
        "@dp.message_handler(func=lambda message: get_current_state(message.chat.id) == States.S_STYLE.value, content_types=types.ContentType.PHOTO)    \n",
        "async def get_style(message: types.Message):\n",
        "    set_state(message.from_user.id, States.S_CONTENT.value)\n",
        "    global image1\n",
        "    image1 = await bot.download_file_by_id(message.photo[-1].file_id)\n",
        "    image1 = Image.open(image1)\n",
        "    # await message.photo[-1].download('data/%s/style.jpg' %\n",
        "    #                               (message.from_user.id))\n",
        "    await message.reply('Отлично, со стилем определились.\\n Теперь отправь мне изображение, стиль которого ты хочешь изменить')\n",
        "\n",
        "@dp.message_handler(func=lambda message: get_current_state(message.chat.id) == States.S_CONTENT.value, content_types=types.ContentType.PHOTO)    \n",
        "async def get_style(message: types.Message):\n",
        "    image2 = await bot.download_file_by_id(message.photo[-1].file_id)\n",
        "    image2 = Image.open(image2)\n",
        "    image3 = image2\n",
        "    await message.reply('Окейсики, теперь подожди немного')\n",
        "    model = StyleTransferModel(device,cnn,cnn_normalization_mean, cnn_normalization_std, image1, image2)\n",
        "    img = image3\n",
        "    unloader = transforms.ToPILImage()\n",
        "    img = model(img)\n",
        "    img2 = unloader(img.squeeze(0))\n",
        "    bio = BytesIO()\n",
        "    bio.name = 'image.jpeg'\n",
        "    img2.save(bio, 'JPEG')\n",
        "    bio.seek(0)\n",
        "    await bot.send_photo(message.from_user.id,photo = bio)\n",
        "    set_state(message.from_user.id, States.S_START.value)\n",
        "\n",
        "async def set_dir(message, type_path):\n",
        "    path = 'data/%s/%s' % (message.from_user.id, type_path)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "@dp.message_handler()\n",
        "async def echo_message(msg: types.Message):\n",
        "    await bot.send_message(msg.from_user.id, msg.text)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    executor.start_polling(dp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start bot with long-polling.\n",
            "Cause exception while getting updates.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiogram/bot/api.py\", line 199, in request\n",
            "    async with session.post(url, data=data, **kwargs) as response:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\", line 1012, in __aenter__\n",
            "    self._resp = await self._coro\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\", line 504, in _request\n",
            "    await resp.start(conn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py\", line 847, in start\n",
            "    message, payload = await self._protocol.read()  # type: ignore  # noqa\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py\", line 591, in read\n",
            "    await self._waiter\n",
            "aiohttp.client_exceptions.ServerDisconnectedError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiogram/dispatcher/__init__.py\", line 243, in start_polling\n",
            "    updates = await self.bot.get_updates(limit=limit, offset=offset, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiogram/bot/bot.py\", line 83, in get_updates\n",
            "    result = await self.request(api.Methods.GET_UPDATES, payload)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiogram/bot/base.py\", line 136, in request\n",
            "    proxy=self.proxy, proxy_auth=self.proxy_auth)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiogram/bot/api.py\", line 202, in request\n",
            "    raise exceptions.NetworkError(f\"aiohttp client throws an error: {e.__class__.__name__}: {e}\")\n",
            "aiogram.utils.exceptions.NetworkError: Aiohttp client throws an error: ServerDisconnectedError:\n",
            "Polling is stopped.\n",
            "/usr/local/lib/python3.6/dist-packages/aiogram/bot/base.py:113: RuntimeWarning: coroutine 'ClientSession.close' was never awaited\n",
            "  session.close()\n",
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f5979c52ac8>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f597c232ba8>, 6819.309944889)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f597c24d2e8>\n",
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f597c2424e0>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f597c2126c8>, 6826.583241403)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f597c24d470>\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimizing..\n",
            "run [50]:\n",
            "Style Loss : 9.749947 Content Loss: 20.823372\n",
            "\n",
            "run [100]:\n",
            "Style Loss : 4.677236 Content Loss: 16.698208\n",
            "\n",
            "run [150]:\n",
            "Style Loss : 3.441935 Content Loss: 15.300850\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 2.906644 Content Loss: 14.645202\n",
            "\n",
            "run [250]:\n",
            "Style Loss : 2.678251 Content Loss: 14.281075\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 2.532545 Content Loss: 14.061394\n",
            "\n",
            "run [350]:\n",
            "Style Loss : 2.456171 Content Loss: 13.923008\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 2.410214 Content Loss: 13.824737\n",
            "\n",
            "run [450]:\n",
            "Style Loss : 2.376648 Content Loss: 13.754292\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 2.363316 Content Loss: 13.698855\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f5979c52c50>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979cca4c0>, 7030.907647803)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f597c24d400>\n",
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f597c24ce80>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979cca388>, 7072.345706009)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f5979c52b70>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimizing..\n",
            "run [50]:\n",
            "Style Loss : 16.983826 Content Loss: 32.090790\n",
            "\n",
            "run [100]:\n",
            "Style Loss : 7.763161 Content Loss: 27.084064\n",
            "\n",
            "run [150]:\n",
            "Style Loss : 4.637348 Content Loss: 25.090958\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 3.403345 Content Loss: 24.020432\n",
            "\n",
            "run [250]:\n",
            "Style Loss : 2.903208 Content Loss: 23.380596\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 2.638815 Content Loss: 22.974009\n",
            "\n",
            "run [350]:\n",
            "Style Loss : 2.487755 Content Loss: 22.687504\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 2.390603 Content Loss: 22.512508\n",
            "\n",
            "run [450]:\n",
            "Style Loss : 2.338969 Content Loss: 22.374727\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 2.307351 Content Loss: 22.279539\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f5979c73f98>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979cca8d0>, 7432.0287757)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f597c242320>\n",
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f597c24d2e8>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979caace0>, 7439.985611289)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f597c249128>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimizing..\n",
            "run [50]:\n",
            "Style Loss : 9.749947 Content Loss: 20.823372\n",
            "\n",
            "run [100]:\n",
            "Style Loss : 4.677236 Content Loss: 16.698208\n",
            "\n",
            "run [150]:\n",
            "Style Loss : 3.441935 Content Loss: 15.300850\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 2.906644 Content Loss: 14.645202\n",
            "\n",
            "run [250]:\n",
            "Style Loss : 2.678251 Content Loss: 14.281075\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 2.532545 Content Loss: 14.061394\n",
            "\n",
            "run [350]:\n",
            "Style Loss : 2.456171 Content Loss: 13.923008\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 2.410214 Content Loss: 13.824737\n",
            "\n",
            "run [450]:\n",
            "Style Loss : 2.376648 Content Loss: 13.754292\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 2.363316 Content Loss: 13.698855\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f5979c78b38>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979caa458>, 8670.735512639)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f5979c78ba8>\n",
            "Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7f597c24d748>\n",
            "Unclosed connector\n",
            "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7f5979cca4c0>, 8993.665806061)]']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7f5979c789b0>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimizing..\n",
            "run [50]:\n",
            "Style Loss : 26.613243 Content Loss: 15.056137\n",
            "\n",
            "run [100]:\n",
            "Style Loss : 18.513912 Content Loss: 14.578157\n",
            "\n",
            "run [150]:\n",
            "Style Loss : 10.449574 Content Loss: 15.282975\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 5.130457 Content Loss: 14.767145\n",
            "\n",
            "run [250]:\n",
            "Style Loss : 3.976939 Content Loss: 14.112865\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 3.655138 Content Loss: 13.847616\n",
            "\n",
            "run [350]:\n",
            "Style Loss : 3.449576 Content Loss: 13.695054\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 3.298433 Content Loss: 13.590355\n",
            "\n",
            "run [450]:\n",
            "Style Loss : 3.191171 Content Loss: 13.511128\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 3.112575 Content Loss: 13.446594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0BDw3EnAVgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}